{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks, 19`ICLR\n",
    "===\n",
    "modified from https://github.com/rahulvigneswaran/Lottery-Ticket-Hypothesis-in-Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import argparse\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorboardX import SummaryWriter\n",
    "import torchvision.utils as vutils\n",
    "import seaborn as sns\n",
    "import torch.nn.init as init\n",
    "import pickle\n",
    "\n",
    "# Custom Libraries\n",
    "import utils\n",
    "\n",
    "# Tensorboard initialization\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Plotting Style\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--lr\",default= 1.2e-3, type=float, help=\"Learning rate\")\n",
    "parser.add_argument(\"--batch_size\", default=128, type=int)\n",
    "parser.add_argument(\"--start_iter\", default=0, type=int)\n",
    "parser.add_argument(\"--end_iter\", default=10, type=int)\n",
    "parser.add_argument(\"--print_freq\", default=1, type=int)\n",
    "parser.add_argument(\"--valid_freq\", default=1, type=int)\n",
    "parser.add_argument(\"--resume\", action=\"store_true\")\n",
    "parser.add_argument(\"--prune_type\", default=\"lt\", type=str, help=\"lt | reinit\")\n",
    "parser.add_argument(\"--gpu\", default=\"0\", type=str)\n",
    "parser.add_argument(\"--dataset\", default=\"cifar10\", type=str, help=\"mnist | cifar10 | fashionmnist | cifar100\")\n",
    "parser.add_argument(\"--arch_type\", default=\"resnet18\", type=str, help=\"fc1 | lenet5 | alexnet | vgg16 | resnet18 | densenet121\")\n",
    "parser.add_argument(\"--prune_percent\", default=10, type=int, help=\"Pruning percent\")\n",
    "parser.add_argument(\"--prune_iterations\", default=5, type=int, help=\"Pruning iterations count\")\n",
    "\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=args.gpu\n",
    "\n",
    "\n",
    "#FIXME resample\n",
    "resample = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prune_type == \"lt\" refers to Lottery Ticket Hypothesis while \"reinit\" refers to reinitialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "reinit = True if args.prune_type==\"reinit\" else False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### again, we're going to use CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "traindataset = datasets.CIFAR10('../datasets/', train=True, download=True,transform=transform)\n",
    "testdataset = datasets.CIFAR10('../datasets/', train=False, transform=transform)      \n",
    "from archs.cifar10 import AlexNet, LeNet5, fc1, vgg, resnet, densenet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(traindataset, batch_size=args.batch_size, shuffle=True, num_workers=0,drop_last=False)\n",
    "test_loader = torch.utils.data.DataLoader(testdataset, batch_size=args.batch_size, shuffle=False, num_workers=0,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "global model\n",
    "model = resnet.resnet18().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defining functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Weight Initalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    '''\n",
    "    Usage:\n",
    "        model = Model()\n",
    "        model.apply(weight_init)\n",
    "    '''\n",
    "    if isinstance(m, nn.Conv1d):\n",
    "        init.normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.Conv3d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.ConvTranspose1d):\n",
    "        init.normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.ConvTranspose2d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.ConvTranspose3d):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.BatchNorm1d):\n",
    "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "        init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "        init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm3d):\n",
    "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
    "        init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        init.xavier_normal_(m.weight.data)\n",
    "        init.normal_(m.bias.data)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "    elif isinstance(m, nn.LSTMCell):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "    elif isinstance(m, nn.GRU):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "    elif isinstance(m, nn.GRUCell):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.apply(weight_init)\n",
    "\n",
    "# Copying and Saving Initial State\n",
    "initial_state_dict = copy.deepcopy(model.state_dict())\n",
    "utils.checkdir(f\"{os.getcwd()}/saves/{args.arch_type}/{args.dataset}/\")\n",
    "torch.save(model, f\"{os.getcwd()}/saves/{args.arch_type}/{args.dataset}/initial_state_dict_{args.prune_type}.pth.tar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recall that in the paper, they emphasized the importance of reusing the initialization of the original model weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Mask Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make an empty mask of the same size as the model\n",
    "def make_mask(model):\n",
    "    global step\n",
    "    global mask\n",
    "    step = 0\n",
    "    for name, param in model.named_parameters(): \n",
    "        if 'weight' in name:\n",
    "            step = step + 1\n",
    "    mask = [None]* step # first, create a stack with None; each element would correspond to each weight in the model\n",
    "    \n",
    "    step = 0\n",
    "    for name, param in model.named_parameters(): \n",
    "        if 'weight' in name:\n",
    "            tensor = param.data.cpu().numpy()\n",
    "            mask[step] = np.ones_like(tensor) # then we filled up with tensor shape same as the corresponding weight, with value 1; just like how we did in the previous example\n",
    "            step = step + 1\n",
    "    step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_mask(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the only difference is they made the mask outside the model whereas the previous implementation puts mask in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. defining optimizer and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss() # Default was F.nll_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. function to recover the original initial weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def original_initialization(mask_temp, initial_state_dict):\n",
    "    global model\n",
    "    \n",
    "    step = 0\n",
    "    for name, param in model.named_parameters(): \n",
    "        if \"weight\" in name: \n",
    "            weight_dev = param.device\n",
    "            param.data = torch.from_numpy(mask_temp[step] * initial_state_dict[name].cpu().numpy()).to(weight_dev)\n",
    "            step = step + 1\n",
    "        if \"bias\" in name:\n",
    "            param.data = initial_state_dict[name]\n",
    "    step = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. functions for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion):\n",
    "    EPS = 1e-6\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.train()\n",
    "    for batch_idx, (imgs, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        #imgs, targets = next(train_loader)\n",
    "        imgs, targets = imgs.to(device), targets.to(device)\n",
    "        output = model(imgs)\n",
    "        train_loss = criterion(output, targets)\n",
    "        train_loss.backward()\n",
    "\n",
    "        # Freezing Pruned weights by making their gradients Zero\n",
    "        for name, p in model.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                tensor = p.data.cpu().numpy()\n",
    "                grad_tensor = p.grad.data.cpu().numpy()\n",
    "                grad_tensor = np.where(tensor < EPS, 0, grad_tensor) # if the weight is lower than EPS, just set to 0\n",
    "                p.grad.data = torch.from_numpy(grad_tensor).to(device)\n",
    "        optimizer.step()\n",
    "    return train_loss.item()\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. function for pruning by percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_by_percentile(percent, resample=False, reinit=False,**kwargs):\n",
    "    global step\n",
    "    global mask\n",
    "    global model\n",
    "\n",
    "    # Calculate percentile value\n",
    "    step = 0\n",
    "    for name, param in model.named_parameters():\n",
    "\n",
    "        # We do not prune bias term\n",
    "        if 'weight' in name:\n",
    "            tensor = param.data.cpu().numpy()\n",
    "            alive = tensor[np.nonzero(tensor)] # flattened array of nonzero values\n",
    "            percentile_value = np.percentile(abs(alive), percent)\n",
    "\n",
    "            # Convert Tensors to numpy and calculate\n",
    "            weight_dev = param.device\n",
    "            new_mask = np.where(abs(tensor) < percentile_value, 0, mask[step])\n",
    "\n",
    "            # Apply new weight and mask\n",
    "            param.data = torch.from_numpy(tensor * new_mask).to(weight_dev)\n",
    "            mask[step] = new_mask\n",
    "            step += 1\n",
    "    step = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE First Pruning Iteration is of No Compression\n",
    "bestacc = 0.0\n",
    "best_accuracy = 0\n",
    "ITERATION = args.prune_iterations\n",
    "comp = np.zeros(ITERATION,float)\n",
    "bestacc = np.zeros(ITERATION,float)\n",
    "step = 0\n",
    "all_loss = np.zeros(args.end_iter,float)\n",
    "all_accuracy = np.zeros(args.end_iter,float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.prune_type=\"reinit\"\n",
    "reinit = True if args.prune_type==\"reinit\" else False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simplification of code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for (pruning iteration):<br>\n",
    " >   if first, pass else prune <br>\n",
    "  >  for (training iteration):<br>\n",
    "   >>     train the model<br>\n",
    "     >>   compute loss and accuracy<br>\n",
    "  >  plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Pruning Level [1:0/5]: ---\n",
      "conv1.weight         | nonzeros =    1728 /    1728 (100.00%) | total_pruned =       0 | shape = (64, 3, 3, 3)\n",
      "bn1.weight           | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "bn1.bias             | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.0.conv1.weight | nonzeros =   36864 /   36864 (100.00%) | total_pruned =       0 | shape = (64, 64, 3, 3)\n",
      "layer1.0.bn1.weight  | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "layer1.0.bn1.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.0.conv2.weight | nonzeros =   36864 /   36864 (100.00%) | total_pruned =       0 | shape = (64, 64, 3, 3)\n",
      "layer1.0.bn2.weight  | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "layer1.0.bn2.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.1.conv1.weight | nonzeros =   36864 /   36864 (100.00%) | total_pruned =       0 | shape = (64, 64, 3, 3)\n",
      "layer1.1.bn1.weight  | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "layer1.1.bn1.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.1.conv2.weight | nonzeros =   36864 /   36864 (100.00%) | total_pruned =       0 | shape = (64, 64, 3, 3)\n",
      "layer1.1.bn2.weight  | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "layer1.1.bn2.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer2.0.conv1.weight | nonzeros =   73727 /   73728 (100.00%) | total_pruned =       1 | shape = (128, 64, 3, 3)\n",
      "layer2.0.bn1.weight  | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "layer2.0.bn1.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.0.conv2.weight | nonzeros =  147456 /  147456 (100.00%) | total_pruned =       0 | shape = (128, 128, 3, 3)\n",
      "layer2.0.bn2.weight  | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "layer2.0.bn2.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.0.shortcut.0.weight | nonzeros =    8192 /    8192 (100.00%) | total_pruned =       0 | shape = (128, 64, 1, 1)\n",
      "layer2.0.shortcut.1.weight | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "layer2.0.shortcut.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.1.conv1.weight | nonzeros =  147456 /  147456 (100.00%) | total_pruned =       0 | shape = (128, 128, 3, 3)\n",
      "layer2.1.bn1.weight  | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "layer2.1.bn1.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.1.conv2.weight | nonzeros =  147456 /  147456 (100.00%) | total_pruned =       0 | shape = (128, 128, 3, 3)\n",
      "layer2.1.bn2.weight  | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "layer2.1.bn2.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer3.0.conv1.weight | nonzeros =  294912 /  294912 (100.00%) | total_pruned =       0 | shape = (256, 128, 3, 3)\n",
      "layer3.0.bn1.weight  | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "layer3.0.bn1.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.0.conv2.weight | nonzeros =  589824 /  589824 (100.00%) | total_pruned =       0 | shape = (256, 256, 3, 3)\n",
      "layer3.0.bn2.weight  | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "layer3.0.bn2.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.0.shortcut.0.weight | nonzeros =   32768 /   32768 (100.00%) | total_pruned =       0 | shape = (256, 128, 1, 1)\n",
      "layer3.0.shortcut.1.weight | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "layer3.0.shortcut.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.1.conv1.weight | nonzeros =  589824 /  589824 (100.00%) | total_pruned =       0 | shape = (256, 256, 3, 3)\n",
      "layer3.1.bn1.weight  | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "layer3.1.bn1.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.1.conv2.weight | nonzeros =  589824 /  589824 (100.00%) | total_pruned =       0 | shape = (256, 256, 3, 3)\n",
      "layer3.1.bn2.weight  | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "layer3.1.bn2.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer4.0.conv1.weight | nonzeros = 1179648 / 1179648 (100.00%) | total_pruned =       0 | shape = (512, 256, 3, 3)\n",
      "layer4.0.bn1.weight  | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (512,)\n",
      "layer4.0.bn1.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.0.conv2.weight | nonzeros = 2359296 / 2359296 (100.00%) | total_pruned =       0 | shape = (512, 512, 3, 3)\n",
      "layer4.0.bn2.weight  | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (512,)\n",
      "layer4.0.bn2.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.0.shortcut.0.weight | nonzeros =  131072 /  131072 (100.00%) | total_pruned =       0 | shape = (512, 256, 1, 1)\n",
      "layer4.0.shortcut.1.weight | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (512,)\n",
      "layer4.0.shortcut.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.1.conv1.weight | nonzeros = 2359296 / 2359296 (100.00%) | total_pruned =       0 | shape = (512, 512, 3, 3)\n",
      "layer4.1.bn1.weight  | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (512,)\n",
      "layer4.1.bn1.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.1.conv2.weight | nonzeros = 2359295 / 2359296 (100.00%) | total_pruned =       1 | shape = (512, 512, 3, 3)\n",
      "layer4.1.bn2.weight  | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (512,)\n",
      "layer4.1.bn2.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "linear.weight        | nonzeros =    5120 /    5120 (100.00%) | total_pruned =       0 | shape = (10, 512)\n",
      "linear.bias          | nonzeros =      10 /      10 (100.00%) | total_pruned =       0 | shape = (10,)\n",
      "alive: 11169160, pruned : 4802, total: 11173962, Compression rate :       1.00x  (  0.04% pruned)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9/10 Loss: 0.869291 Accuracy: 56.35% Best Accuracy: 56.35%: 100%|██████████| 10/10 [08:09<00:00, 48.99s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Pruning Level [1:1/5]: ---\n",
      "conv1.weight         | nonzeros =    1555 /    1728 ( 89.99%) | total_pruned =     173 | shape = (64, 3, 3, 3)\n",
      "bn1.weight           | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "bn1.bias             | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.0.conv1.weight | nonzeros =   33177 /   36864 ( 90.00%) | total_pruned =    3687 | shape = (64, 64, 3, 3)\n",
      "layer1.0.bn1.weight  | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "layer1.0.bn1.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.0.conv2.weight | nonzeros =   33178 /   36864 ( 90.00%) | total_pruned =    3686 | shape = (64, 64, 3, 3)\n",
      "layer1.0.bn2.weight  | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "layer1.0.bn2.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.1.conv1.weight | nonzeros =   33177 /   36864 ( 90.00%) | total_pruned =    3687 | shape = (64, 64, 3, 3)\n",
      "layer1.1.bn1.weight  | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "layer1.1.bn1.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.1.conv2.weight | nonzeros =   33177 /   36864 ( 90.00%) | total_pruned =    3687 | shape = (64, 64, 3, 3)\n",
      "layer1.1.bn2.weight  | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "layer1.1.bn2.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer2.0.conv1.weight | nonzeros =   66353 /   73728 ( 90.00%) | total_pruned =    7375 | shape = (128, 64, 3, 3)\n",
      "layer2.0.bn1.weight  | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "layer2.0.bn1.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.0.conv2.weight | nonzeros =  132711 /  147456 ( 90.00%) | total_pruned =   14745 | shape = (128, 128, 3, 3)\n",
      "layer2.0.bn2.weight  | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "layer2.0.bn2.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.0.shortcut.0.weight | nonzeros =    7373 /    8192 ( 90.00%) | total_pruned =     819 | shape = (128, 64, 1, 1)\n",
      "layer2.0.shortcut.1.weight | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "layer2.0.shortcut.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.1.conv1.weight | nonzeros =  132709 /  147456 ( 90.00%) | total_pruned =   14747 | shape = (128, 128, 3, 3)\n",
      "layer2.1.bn1.weight  | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "layer2.1.bn1.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.1.conv2.weight | nonzeros =  132709 /  147456 ( 90.00%) | total_pruned =   14747 | shape = (128, 128, 3, 3)\n",
      "layer2.1.bn2.weight  | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "layer2.1.bn2.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer3.0.conv1.weight | nonzeros =  265421 /  294912 ( 90.00%) | total_pruned =   29491 | shape = (256, 128, 3, 3)\n",
      "layer3.0.bn1.weight  | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "layer3.0.bn1.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.0.conv2.weight | nonzeros =  530842 /  589824 ( 90.00%) | total_pruned =   58982 | shape = (256, 256, 3, 3)\n",
      "layer3.0.bn2.weight  | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "layer3.0.bn2.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.0.shortcut.0.weight | nonzeros =   29491 /   32768 ( 90.00%) | total_pruned =    3277 | shape = (256, 128, 1, 1)\n",
      "layer3.0.shortcut.1.weight | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "layer3.0.shortcut.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.1.conv1.weight | nonzeros =  530843 /  589824 ( 90.00%) | total_pruned =   58981 | shape = (256, 256, 3, 3)\n",
      "layer3.1.bn1.weight  | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "layer3.1.bn1.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.1.conv2.weight | nonzeros =  530841 /  589824 ( 90.00%) | total_pruned =   58983 | shape = (256, 256, 3, 3)\n",
      "layer3.1.bn2.weight  | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "layer3.1.bn2.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer4.0.conv1.weight | nonzeros = 1061683 / 1179648 ( 90.00%) | total_pruned =  117965 | shape = (512, 256, 3, 3)\n",
      "layer4.0.bn1.weight  | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "layer4.0.bn1.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.0.conv2.weight | nonzeros = 2123366 / 2359296 ( 90.00%) | total_pruned =  235930 | shape = (512, 512, 3, 3)\n",
      "layer4.0.bn2.weight  | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "layer4.0.bn2.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.0.shortcut.0.weight | nonzeros =  117966 /  131072 ( 90.00%) | total_pruned =   13106 | shape = (512, 256, 1, 1)\n",
      "layer4.0.shortcut.1.weight | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "layer4.0.shortcut.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.1.conv1.weight | nonzeros = 2123379 / 2359296 ( 90.00%) | total_pruned =  235917 | shape = (512, 512, 3, 3)\n",
      "layer4.1.bn1.weight  | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "layer4.1.bn1.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.1.conv2.weight | nonzeros = 2123363 / 2359296 ( 90.00%) | total_pruned =  235933 | shape = (512, 512, 3, 3)\n",
      "layer4.1.bn2.weight  | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "layer4.1.bn2.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "linear.weight        | nonzeros =    4608 /    5120 ( 90.00%) | total_pruned =     512 | shape = (10, 512)\n",
      "linear.bias          | nonzeros =      10 /      10 (100.00%) | total_pruned =       0 | shape = (10,)\n",
      "alive: 10052242, pruned : 1121720, total: 11173962, Compression rate :       1.11x  ( 10.04% pruned)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9/10 Loss: 1.107746 Accuracy: 57.61% Best Accuracy: 57.61%: 100%|██████████| 10/10 [07:58<00:00, 47.83s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Pruning Level [1:2/5]: ---\n",
      "conv1.weight         | nonzeros =    1399 /    1728 ( 80.96%) | total_pruned =     329 | shape = (64, 3, 3, 3)\n",
      "bn1.weight           | nonzeros =      51 /      64 ( 79.69%) | total_pruned =      13 | shape = (64,)\n",
      "bn1.bias             | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.0.conv1.weight | nonzeros =   29859 /   36864 ( 81.00%) | total_pruned =    7005 | shape = (64, 64, 3, 3)\n",
      "layer1.0.bn1.weight  | nonzeros =      51 /      64 ( 79.69%) | total_pruned =      13 | shape = (64,)\n",
      "layer1.0.bn1.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.0.conv2.weight | nonzeros =   29860 /   36864 ( 81.00%) | total_pruned =    7004 | shape = (64, 64, 3, 3)\n",
      "layer1.0.bn2.weight  | nonzeros =      51 /      64 ( 79.69%) | total_pruned =      13 | shape = (64,)\n",
      "layer1.0.bn2.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.1.conv1.weight | nonzeros =   29859 /   36864 ( 81.00%) | total_pruned =    7005 | shape = (64, 64, 3, 3)\n",
      "layer1.1.bn1.weight  | nonzeros =      51 /      64 ( 79.69%) | total_pruned =      13 | shape = (64,)\n",
      "layer1.1.bn1.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.1.conv2.weight | nonzeros =   29859 /   36864 ( 81.00%) | total_pruned =    7005 | shape = (64, 64, 3, 3)\n",
      "layer1.1.bn2.weight  | nonzeros =      51 /      64 ( 79.69%) | total_pruned =      13 | shape = (64,)\n",
      "layer1.1.bn2.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer2.0.conv1.weight | nonzeros =   59718 /   73728 ( 81.00%) | total_pruned =   14010 | shape = (128, 64, 3, 3)\n",
      "layer2.0.bn1.weight  | nonzeros =     103 /     128 ( 80.47%) | total_pruned =      25 | shape = (128,)\n",
      "layer2.0.bn1.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.0.conv2.weight | nonzeros =  119440 /  147456 ( 81.00%) | total_pruned =   28016 | shape = (128, 128, 3, 3)\n",
      "layer2.0.bn2.weight  | nonzeros =     103 /     128 ( 80.47%) | total_pruned =      25 | shape = (128,)\n",
      "layer2.0.bn2.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.0.shortcut.0.weight | nonzeros =    6635 /    8192 ( 80.99%) | total_pruned =    1557 | shape = (128, 64, 1, 1)\n",
      "layer2.0.shortcut.1.weight | nonzeros =     103 /     128 ( 80.47%) | total_pruned =      25 | shape = (128,)\n",
      "layer2.0.shortcut.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.1.conv1.weight | nonzeros =  119437 /  147456 ( 81.00%) | total_pruned =   28019 | shape = (128, 128, 3, 3)\n",
      "layer2.1.bn1.weight  | nonzeros =     103 /     128 ( 80.47%) | total_pruned =      25 | shape = (128,)\n",
      "layer2.1.bn1.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.1.conv2.weight | nonzeros =  119438 /  147456 ( 81.00%) | total_pruned =   28018 | shape = (128, 128, 3, 3)\n",
      "layer2.1.bn2.weight  | nonzeros =     103 /     128 ( 80.47%) | total_pruned =      25 | shape = (128,)\n",
      "layer2.1.bn2.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer3.0.conv1.weight | nonzeros =  238879 /  294912 ( 81.00%) | total_pruned =   56033 | shape = (256, 128, 3, 3)\n",
      "layer3.0.bn1.weight  | nonzeros =     207 /     256 ( 80.86%) | total_pruned =      49 | shape = (256,)\n",
      "layer3.0.bn1.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.0.conv2.weight | nonzeros =  477757 /  589824 ( 81.00%) | total_pruned =  112067 | shape = (256, 256, 3, 3)\n",
      "layer3.0.bn2.weight  | nonzeros =     207 /     256 ( 80.86%) | total_pruned =      49 | shape = (256,)\n",
      "layer3.0.bn2.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.0.shortcut.0.weight | nonzeros =   26542 /   32768 ( 81.00%) | total_pruned =    6226 | shape = (256, 128, 1, 1)\n",
      "layer3.0.shortcut.1.weight | nonzeros =     207 /     256 ( 80.86%) | total_pruned =      49 | shape = (256,)\n",
      "layer3.0.shortcut.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.1.conv1.weight | nonzeros =  477756 /  589824 ( 81.00%) | total_pruned =  112068 | shape = (256, 256, 3, 3)\n",
      "layer3.1.bn1.weight  | nonzeros =     207 /     256 ( 80.86%) | total_pruned =      49 | shape = (256,)\n",
      "layer3.1.bn1.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.1.conv2.weight | nonzeros =  477758 /  589824 ( 81.00%) | total_pruned =  112066 | shape = (256, 256, 3, 3)\n",
      "layer3.1.bn2.weight  | nonzeros =     207 /     256 ( 80.86%) | total_pruned =      49 | shape = (256,)\n",
      "layer3.1.bn2.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer4.0.conv1.weight | nonzeros =  955524 / 1179648 ( 81.00%) | total_pruned =  224124 | shape = (512, 256, 3, 3)\n",
      "layer4.0.bn1.weight  | nonzeros =     414 /     512 ( 80.86%) | total_pruned =      98 | shape = (512,)\n",
      "layer4.0.bn1.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.0.conv2.weight | nonzeros = 1911030 / 2359296 ( 81.00%) | total_pruned =  448266 | shape = (512, 512, 3, 3)\n",
      "layer4.0.bn2.weight  | nonzeros =     414 /     512 ( 80.86%) | total_pruned =      98 | shape = (512,)\n",
      "layer4.0.bn2.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.0.shortcut.0.weight | nonzeros =  106169 /  131072 ( 81.00%) | total_pruned =   24903 | shape = (512, 256, 1, 1)\n",
      "layer4.0.shortcut.1.weight | nonzeros =     414 /     512 ( 80.86%) | total_pruned =      98 | shape = (512,)\n",
      "layer4.0.shortcut.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.1.conv1.weight | nonzeros = 1911037 / 2359296 ( 81.00%) | total_pruned =  448259 | shape = (512, 512, 3, 3)\n",
      "layer4.1.bn1.weight  | nonzeros =     414 /     512 ( 80.86%) | total_pruned =      98 | shape = (512,)\n",
      "layer4.1.bn1.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.1.conv2.weight | nonzeros = 1911025 / 2359296 ( 81.00%) | total_pruned =  448271 | shape = (512, 512, 3, 3)\n",
      "layer4.1.bn2.weight  | nonzeros =     414 /     512 ( 80.86%) | total_pruned =      98 | shape = (512,)\n",
      "layer4.1.bn2.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "linear.weight        | nonzeros =    4147 /    5120 ( 81.00%) | total_pruned =     973 | shape = (10, 512)\n",
      "linear.bias          | nonzeros =      10 /      10 (100.00%) | total_pruned =       0 | shape = (10,)\n",
      "alive: 9047013, pruned : 2126949, total: 11173962, Compression rate :       1.24x  ( 19.03% pruned)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9/10 Loss: 1.097351 Accuracy: 42.87% Best Accuracy: 58.34%: 100%|██████████| 10/10 [07:51<00:00, 47.14s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Pruning Level [1:3/5]: ---\n",
      "conv1.weight         | nonzeros =    1259 /    1728 ( 72.86%) | total_pruned =     469 | shape = (64, 3, 3, 3)\n",
      "bn1.weight           | nonzeros =      46 /      64 ( 71.88%) | total_pruned =      18 | shape = (64,)\n",
      "bn1.bias             | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.0.conv1.weight | nonzeros =   26873 /   36864 ( 72.90%) | total_pruned =    9991 | shape = (64, 64, 3, 3)\n",
      "layer1.0.bn1.weight  | nonzeros =      46 /      64 ( 71.88%) | total_pruned =      18 | shape = (64,)\n",
      "layer1.0.bn1.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.0.conv2.weight | nonzeros =   26873 /   36864 ( 72.90%) | total_pruned =    9991 | shape = (64, 64, 3, 3)\n",
      "layer1.0.bn2.weight  | nonzeros =      46 /      64 ( 71.88%) | total_pruned =      18 | shape = (64,)\n",
      "layer1.0.bn2.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.1.conv1.weight | nonzeros =   26873 /   36864 ( 72.90%) | total_pruned =    9991 | shape = (64, 64, 3, 3)\n",
      "layer1.1.bn1.weight  | nonzeros =      46 /      64 ( 71.88%) | total_pruned =      18 | shape = (64,)\n",
      "layer1.1.bn1.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.1.conv2.weight | nonzeros =   26873 /   36864 ( 72.90%) | total_pruned =    9991 | shape = (64, 64, 3, 3)\n",
      "layer1.1.bn2.weight  | nonzeros =      46 /      64 ( 71.88%) | total_pruned =      18 | shape = (64,)\n",
      "layer1.1.bn2.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer2.0.conv1.weight | nonzeros =   53745 /   73728 ( 72.90%) | total_pruned =   19983 | shape = (128, 64, 3, 3)\n",
      "layer2.0.bn1.weight  | nonzeros =      92 /     128 ( 71.88%) | total_pruned =      36 | shape = (128,)\n",
      "layer2.0.bn1.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.0.conv2.weight | nonzeros =  107495 /  147456 ( 72.90%) | total_pruned =   39961 | shape = (128, 128, 3, 3)\n",
      "layer2.0.bn2.weight  | nonzeros =      92 /     128 ( 71.88%) | total_pruned =      36 | shape = (128,)\n",
      "layer2.0.bn2.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.0.shortcut.0.weight | nonzeros =    5971 /    8192 ( 72.89%) | total_pruned =    2221 | shape = (128, 64, 1, 1)\n",
      "layer2.0.shortcut.1.weight | nonzeros =      92 /     128 ( 71.88%) | total_pruned =      36 | shape = (128,)\n",
      "layer2.0.shortcut.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.1.conv1.weight | nonzeros =  107493 /  147456 ( 72.90%) | total_pruned =   39963 | shape = (128, 128, 3, 3)\n",
      "layer2.1.bn1.weight  | nonzeros =      92 /     128 ( 71.88%) | total_pruned =      36 | shape = (128,)\n",
      "layer2.1.bn1.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.1.conv2.weight | nonzeros =  107493 /  147456 ( 72.90%) | total_pruned =   39963 | shape = (128, 128, 3, 3)\n",
      "layer2.1.bn2.weight  | nonzeros =      92 /     128 ( 71.88%) | total_pruned =      36 | shape = (128,)\n",
      "layer2.1.bn2.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer3.0.conv1.weight | nonzeros =  214993 /  294912 ( 72.90%) | total_pruned =   79919 | shape = (256, 128, 3, 3)\n",
      "layer3.0.bn1.weight  | nonzeros =     186 /     256 ( 72.66%) | total_pruned =      70 | shape = (256,)\n",
      "layer3.0.bn1.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.0.conv2.weight | nonzeros =  429980 /  589824 ( 72.90%) | total_pruned =  159844 | shape = (256, 256, 3, 3)\n",
      "layer3.0.bn2.weight  | nonzeros =     186 /     256 ( 72.66%) | total_pruned =      70 | shape = (256,)\n",
      "layer3.0.bn2.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.0.shortcut.0.weight | nonzeros =   23888 /   32768 ( 72.90%) | total_pruned =    8880 | shape = (256, 128, 1, 1)\n",
      "layer3.0.shortcut.1.weight | nonzeros =     186 /     256 ( 72.66%) | total_pruned =      70 | shape = (256,)\n",
      "layer3.0.shortcut.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.1.conv1.weight | nonzeros =  429980 /  589824 ( 72.90%) | total_pruned =  159844 | shape = (256, 256, 3, 3)\n",
      "layer3.1.bn1.weight  | nonzeros =     186 /     256 ( 72.66%) | total_pruned =      70 | shape = (256,)\n",
      "layer3.1.bn1.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.1.conv2.weight | nonzeros =  429981 /  589824 ( 72.90%) | total_pruned =  159843 | shape = (256, 256, 3, 3)\n",
      "layer3.1.bn2.weight  | nonzeros =     186 /     256 ( 72.66%) | total_pruned =      70 | shape = (256,)\n",
      "layer3.1.bn2.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer4.0.conv1.weight | nonzeros =  859981 / 1179648 ( 72.90%) | total_pruned =  319667 | shape = (512, 256, 3, 3)\n",
      "layer4.0.bn1.weight  | nonzeros =     372 /     512 ( 72.66%) | total_pruned =     140 | shape = (512,)\n",
      "layer4.0.bn1.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.0.conv2.weight | nonzeros = 1719926 / 2359296 ( 72.90%) | total_pruned =  639370 | shape = (512, 512, 3, 3)\n",
      "layer4.0.bn2.weight  | nonzeros =     372 /     512 ( 72.66%) | total_pruned =     140 | shape = (512,)\n",
      "layer4.0.bn2.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.0.shortcut.0.weight | nonzeros =   95552 /  131072 ( 72.90%) | total_pruned =   35520 | shape = (512, 256, 1, 1)\n",
      "layer4.0.shortcut.1.weight | nonzeros =     372 /     512 ( 72.66%) | total_pruned =     140 | shape = (512,)\n",
      "layer4.0.shortcut.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.1.conv1.weight | nonzeros = 1719935 / 2359296 ( 72.90%) | total_pruned =  639361 | shape = (512, 512, 3, 3)\n",
      "layer4.1.bn1.weight  | nonzeros =     372 /     512 ( 72.66%) | total_pruned =     140 | shape = (512,)\n",
      "layer4.1.bn1.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.1.conv2.weight | nonzeros = 1719921 / 2359296 ( 72.90%) | total_pruned =  639375 | shape = (512, 512, 3, 3)\n",
      "layer4.1.bn2.weight  | nonzeros =     372 /     512 ( 72.66%) | total_pruned =     140 | shape = (512,)\n",
      "layer4.1.bn2.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "linear.weight        | nonzeros =    3732 /    5120 ( 72.89%) | total_pruned =    1388 | shape = (10, 512)\n",
      "linear.bias          | nonzeros =      10 /      10 (100.00%) | total_pruned =       0 | shape = (10,)\n",
      "alive: 8142307, pruned : 3031655, total: 11173962, Compression rate :       1.37x  ( 27.13% pruned)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9/10 Loss: 0.909547 Accuracy: 62.28% Best Accuracy: 62.28%: 100%|██████████| 10/10 [07:45<00:00, 46.53s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Pruning Level [1:4/5]: ---\n",
      "conv1.weight         | nonzeros =    1133 /    1728 ( 65.57%) | total_pruned =     595 | shape = (64, 3, 3, 3)\n",
      "bn1.weight           | nonzeros =      41 /      64 ( 64.06%) | total_pruned =      23 | shape = (64,)\n",
      "bn1.bias             | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.0.conv1.weight | nonzeros =   24185 /   36864 ( 65.61%) | total_pruned =   12679 | shape = (64, 64, 3, 3)\n",
      "layer1.0.bn1.weight  | nonzeros =      41 /      64 ( 64.06%) | total_pruned =      23 | shape = (64,)\n",
      "layer1.0.bn1.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.0.conv2.weight | nonzeros =   24185 /   36864 ( 65.61%) | total_pruned =   12679 | shape = (64, 64, 3, 3)\n",
      "layer1.0.bn2.weight  | nonzeros =      41 /      64 ( 64.06%) | total_pruned =      23 | shape = (64,)\n",
      "layer1.0.bn2.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.1.conv1.weight | nonzeros =   24185 /   36864 ( 65.61%) | total_pruned =   12679 | shape = (64, 64, 3, 3)\n",
      "layer1.1.bn1.weight  | nonzeros =      41 /      64 ( 64.06%) | total_pruned =      23 | shape = (64,)\n",
      "layer1.1.bn1.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.1.conv2.weight | nonzeros =   24186 /   36864 ( 65.61%) | total_pruned =   12678 | shape = (64, 64, 3, 3)\n",
      "layer1.1.bn2.weight  | nonzeros =      41 /      64 ( 64.06%) | total_pruned =      23 | shape = (64,)\n",
      "layer1.1.bn2.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer2.0.conv1.weight | nonzeros =   48370 /   73728 ( 65.61%) | total_pruned =   25358 | shape = (128, 64, 3, 3)\n",
      "layer2.0.bn1.weight  | nonzeros =      82 /     128 ( 64.06%) | total_pruned =      46 | shape = (128,)\n",
      "layer2.0.bn1.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.0.conv2.weight | nonzeros =   96746 /  147456 ( 65.61%) | total_pruned =   50710 | shape = (128, 128, 3, 3)\n",
      "layer2.0.bn2.weight  | nonzeros =      82 /     128 ( 64.06%) | total_pruned =      46 | shape = (128,)\n",
      "layer2.0.bn2.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.0.shortcut.0.weight | nonzeros =    5374 /    8192 ( 65.60%) | total_pruned =    2818 | shape = (128, 64, 1, 1)\n",
      "layer2.0.shortcut.1.weight | nonzeros =      82 /     128 ( 64.06%) | total_pruned =      46 | shape = (128,)\n",
      "layer2.0.shortcut.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.1.conv1.weight | nonzeros =   96742 /  147456 ( 65.61%) | total_pruned =   50714 | shape = (128, 128, 3, 3)\n",
      "layer2.1.bn1.weight  | nonzeros =      82 /     128 ( 64.06%) | total_pruned =      46 | shape = (128,)\n",
      "layer2.1.bn1.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.1.conv2.weight | nonzeros =   96743 /  147456 ( 65.61%) | total_pruned =   50713 | shape = (128, 128, 3, 3)\n",
      "layer2.1.bn2.weight  | nonzeros =      82 /     128 ( 64.06%) | total_pruned =      46 | shape = (128,)\n",
      "layer2.1.bn2.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer3.0.conv1.weight | nonzeros =  193493 /  294912 ( 65.61%) | total_pruned =  101419 | shape = (256, 128, 3, 3)\n",
      "layer3.0.bn1.weight  | nonzeros =     167 /     256 ( 65.23%) | total_pruned =      89 | shape = (256,)\n",
      "layer3.0.bn1.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.0.conv2.weight | nonzeros =  386985 /  589824 ( 65.61%) | total_pruned =  202839 | shape = (256, 256, 3, 3)\n",
      "layer3.0.bn2.weight  | nonzeros =     167 /     256 ( 65.23%) | total_pruned =      89 | shape = (256,)\n",
      "layer3.0.bn2.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.0.shortcut.0.weight | nonzeros =   21499 /   32768 ( 65.61%) | total_pruned =   11269 | shape = (256, 128, 1, 1)\n",
      "layer3.0.shortcut.1.weight | nonzeros =     167 /     256 ( 65.23%) | total_pruned =      89 | shape = (256,)\n",
      "layer3.0.shortcut.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.1.conv1.weight | nonzeros =  386981 /  589824 ( 65.61%) | total_pruned =  202843 | shape = (256, 256, 3, 3)\n",
      "layer3.1.bn1.weight  | nonzeros =     167 /     256 ( 65.23%) | total_pruned =      89 | shape = (256,)\n",
      "layer3.1.bn1.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.1.conv2.weight | nonzeros =  386982 /  589824 ( 65.61%) | total_pruned =  202842 | shape = (256, 256, 3, 3)\n",
      "layer3.1.bn2.weight  | nonzeros =     167 /     256 ( 65.23%) | total_pruned =      89 | shape = (256,)\n",
      "layer3.1.bn2.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer4.0.conv1.weight | nonzeros =  773983 / 1179648 ( 65.61%) | total_pruned =  405665 | shape = (512, 256, 3, 3)\n",
      "layer4.0.bn1.weight  | nonzeros =     334 /     512 ( 65.23%) | total_pruned =     178 | shape = (512,)\n",
      "layer4.0.bn1.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.0.conv2.weight | nonzeros = 1547933 / 2359296 ( 65.61%) | total_pruned =  811363 | shape = (512, 512, 3, 3)\n",
      "layer4.0.bn2.weight  | nonzeros =     334 /     512 ( 65.23%) | total_pruned =     178 | shape = (512,)\n",
      "layer4.0.bn2.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.0.shortcut.0.weight | nonzeros =   85997 /  131072 ( 65.61%) | total_pruned =   45075 | shape = (512, 256, 1, 1)\n",
      "layer4.0.shortcut.1.weight | nonzeros =     334 /     512 ( 65.23%) | total_pruned =     178 | shape = (512,)\n",
      "layer4.0.shortcut.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.1.conv1.weight | nonzeros = 1547938 / 2359296 ( 65.61%) | total_pruned =  811358 | shape = (512, 512, 3, 3)\n",
      "layer4.1.bn1.weight  | nonzeros =     334 /     512 ( 65.23%) | total_pruned =     178 | shape = (512,)\n",
      "layer4.1.bn1.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.1.conv2.weight | nonzeros = 1547929 / 2359296 ( 65.61%) | total_pruned =  811367 | shape = (512, 512, 3, 3)\n",
      "layer4.1.bn2.weight  | nonzeros =     334 /     512 ( 65.23%) | total_pruned =     178 | shape = (512,)\n",
      "layer4.1.bn2.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "linear.weight        | nonzeros =    3358 /    5120 ( 65.59%) | total_pruned =    1762 | shape = (10, 512)\n",
      "linear.bias          | nonzeros =      10 /      10 (100.00%) | total_pruned =       0 | shape = (10,)\n",
      "alive: 7328057, pruned : 3845905, total: 11173962, Compression rate :       1.52x  ( 34.42% pruned)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9/10 Loss: 0.733524 Accuracy: 44.69% Best Accuracy: 59.25%: 100%|██████████| 10/10 [07:39<00:00, 45.92s/it]\n"
     ]
    }
   ],
   "source": [
    "for _ite in range(args.start_iter, ITERATION):\n",
    "    if not _ite == 0: # we first train;, then prune\n",
    "        prune_by_percentile(args.prune_percent, resample=resample, reinit=reinit)\n",
    "        if reinit:\n",
    "            model.apply(weight_init)\n",
    "            step = 0\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    weight_dev = param.device\n",
    "                    param.data = torch.from_numpy(param.data.cpu().numpy() * mask[step]).to(weight_dev)\n",
    "                    step = step + 1\n",
    "            step = 0\n",
    "        else:\n",
    "            original_initialization(mask, initial_state_dict)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
    "    print(f\"\\n--- Pruning Level [{1}:{_ite}/{ITERATION}]: ---\")\n",
    "\n",
    "    # Print the table of Nonzeros in each layer\n",
    "    comp1 = utils.print_nonzeros(model)\n",
    "    comp[_ite] = comp1\n",
    "    pbar = tqdm(range(args.end_iter))\n",
    "\n",
    "    for iter_ in pbar:\n",
    "\n",
    "        # Frequency for Testing\n",
    "        if iter_ % args.valid_freq == 0:\n",
    "            accuracy = test(model, test_loader, criterion)\n",
    "\n",
    "            # Save Weights\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                utils.checkdir(f\"{os.getcwd()}/saves/{args.arch_type}/{args.dataset}/\")\n",
    "                torch.save(model,f\"{os.getcwd()}/saves/{args.arch_type}/{args.dataset}/{_ite}_model_{args.prune_type}.pth.tar\")\n",
    "\n",
    "        # Training\n",
    "        loss = train(model, train_loader, optimizer, criterion)\n",
    "        all_loss[iter_] = loss\n",
    "        all_accuracy[iter_] = accuracy\n",
    "\n",
    "        # Frequency for Printing Accuracy and Loss\n",
    "        if iter_ % args.print_freq == 0:\n",
    "            pbar.set_description(\n",
    "                f'Train Epoch: {iter_}/{args.end_iter} Loss: {loss:.6f} Accuracy: {accuracy:.2f}% Best Accuracy: {best_accuracy:.2f}%')       \n",
    "\n",
    "    writer.add_scalar('Accuracy/test', best_accuracy, comp1)\n",
    "    bestacc[_ite]=best_accuracy\n",
    "    \n",
    "    # Plotting Loss (Training), Accuracy (Testing), Iteration Curve\n",
    "    #NOTE Loss is computed for every iteration while Accuracy is computed only for every {args.valid_freq} iterations. Therefore Accuracy saved is constant during the uncomputed iterations.\n",
    "    #NOTE Normalized the accuracy to [0,100] for ease of plotting.\n",
    "    plt.plot(np.arange(1,(args.end_iter)+1), 100*(all_loss - np.min(all_loss))/np.ptp(all_loss).astype(float), c=\"blue\", label=\"Loss\") \n",
    "    plt.plot(np.arange(1,(args.end_iter)+1), all_accuracy, c=\"red\", label=\"Accuracy\") \n",
    "    plt.title(f\"Loss Vs Accuracy Vs Iterations ({args.dataset},{args.arch_type})\") \n",
    "    plt.xlabel(\"Iterations\") \n",
    "    plt.ylabel(\"Loss and Accuracy\") \n",
    "    plt.legend() \n",
    "    plt.grid(color=\"gray\") \n",
    "    utils.checkdir(f\"{os.getcwd()}/plots/lt/{args.arch_type}/{args.dataset}/\")\n",
    "    plt.savefig(f\"{os.getcwd()}/plots/lt/{args.arch_type}/{args.dataset}/{args.prune_type}_LossVsAccuracy_{comp1}.png\", dpi=1200) \n",
    "    plt.close()\n",
    "\n",
    "    # Dump Plot values\n",
    "    utils.checkdir(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/\")\n",
    "    all_loss.dump(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/{args.prune_type}_all_loss_{comp1}.dat\")\n",
    "    all_accuracy.dump(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/{args.prune_type}_all_accuracy_{comp1}.dat\")\n",
    "\n",
    "    # Dumping mask\n",
    "    utils.checkdir(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/\")\n",
    "    with open(f\"{os.getcwd()}/dumps/lt/{args.arch_type}/{args.dataset}/{args.prune_type}_mask_{comp1}.pkl\", 'wb') as fp:\n",
    "        pickle.dump(mask, fp)\n",
    "\n",
    "    # Making variables into 0\n",
    "    best_accuracy = 0\n",
    "    all_loss = np.zeros(args.end_iter,float)\n",
    "    all_accuracy = np.zeros(args.end_iter,float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping Values for Plotting\n",
    "utils.checkdir(f\"{os.getcwd()}/dumps/{args.prune_type}/{args.arch_type}/{args.dataset}/\")\n",
    "comp.dump(f\"{os.getcwd()}/dumps/{args.prune_type}/{args.arch_type}/{args.dataset}/{args.prune_type}_compression.dat\")\n",
    "bestacc.dump(f\"{os.getcwd()}/dumps/{args.prune_type}/{args.arch_type}/{args.dataset}/{args.prune_type}_bestaccuracy.dat\")\n",
    "\n",
    "# Plotting\n",
    "a = np.arange(args.prune_iterations)\n",
    "plt.plot(a, bestacc, c=\"blue\", label=\"Winning tickets\") \n",
    "plt.title(f\"Test Accuracy vs Unpruned Weights Percentage ({args.dataset},{args.arch_type})\") \n",
    "plt.xlabel(\"Unpruned Weights Percentage\") \n",
    "plt.ylabel(\"test accuracy\") \n",
    "plt.xticks(a, comp, rotation =\"vertical\") \n",
    "plt.ylim(0,100)\n",
    "plt.legend() \n",
    "plt.grid(color=\"gray\") \n",
    "utils.checkdir(f\"{os.getcwd()}/plots/{args.prune_type}/{args.arch_type}/{args.dataset}/\")\n",
    "plt.savefig(f\"{os.getcwd()}/plots/{args.prune_type}/{args.arch_type}/{args.dataset}/{args.prune_type}_AccuracyVsWeights.png\", dpi=1200) \n",
    "plt.close() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pruning",
   "language": "python",
   "name": "pruning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
